# llama-clone-freecodecamp
 This is my implementation of a transformer-based language model based on the  [FreeCodeCamp tutorial](https://www.youtube.com/watch?v=biveB0gOlak). The project helped me understand the inner workings of LLaMA-style architecture, inlcuding:
 -tokenization
 -multi-head self-attention
 -transformer blocks
 language modeling objectives

## What I Learned

## Future Plans

## Credits

Tutorial by FreeCodeCamp. My version includes personal notes, refactoring, and planned experiments.

